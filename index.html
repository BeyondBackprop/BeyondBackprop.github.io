<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Beyond Backprop - NeurIPS Workshop 2020</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta content="" name="keywords">
  <meta content="" name="description">

  <!-- Favicons -->
  <link href="img/favicon.jpg" rel="icon">
  <link href="img/favicon.jpg" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Poppins:300,400,500,700" rel="stylesheet">

  <!-- Bootstrap CSS File -->
  <link href="lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Libraries CSS Files -->
  <link href="lib/font-awesome/css/font-awesome.min.css" rel="stylesheet">
  <link href="lib/animate/animate.min.css" rel="stylesheet">

  <!-- Main Stylesheet File -->
  <link href="css/style.css" rel="stylesheet">

  <!-- =======================================================
    Theme Name: Regna
    Theme URL: https://bootstrapmade.com/regna-bootstrap-onepage-template/
    Author: BootstrapMade.com
    License: https://bootstrapmade.com/license/
  ======================================================= -->
</head>

<body>

  <!--==========================
  Header
  ============================-->
  <header id="header">
    <div class="container">

<!--       <div class="pull-left">
        <h1><a href="#hero">Beyond Backprop</a></h1>
      </div> -->

      <nav id="nav-menu-container">
        <ul class="nav-menu">
          <li class="menu-active"><a href="#hero">Home</a></li>
          <li><a href="#about">Overview</a></li>
          <li><a href="#call-to-action">Schedule</a></li>
          <li><a href="#speakers">Speakers</a></li>
          <li><a href="#organizers">Organizers</a></li>
          <li><a href="#papers">Accepted Papers</a></li>
        </ul>
      </nav><!-- #nav-menu-container -->
    </div>
  </header><!-- #header -->


  <!--==========================
    Hero Section
  ============================-->
  <section id="hero">
    <div class="hero-container">
      <h1>Beyond Backpropagation</h1>
      <h2>Novel Ideas for Training Neural Architectures</h2>
      <h2>Workshop at NeurIPS, 12th of December 2020</h2>
      <!-- <a href="#about" class="btn-get-started">Get Started</a> -->
    </div>
  </section><!-- #hero -->

  <main id="main">

    <!--==========================
      About Us Section
    ============================-->
    <section id="about">
      <div class="container">
        <div class="row about-container">
          <!-- <div class="row justify-content-center"> -->

          <div class="col-lg-12 content order-lg-9 order-2">
            <h2 class="title">Overview</h2>
            <p>
              Is backpropagation the ultimate tool on the path to achieving artificial intelligence as its success and widespread adoption would suggest?
            </p>
            <p>
              Many have questioned the biological plausibility of backpropagation as a learning mechanism since its discovery. 
              The weight transport and timing problems are the most disputable. The same properties of backpropagation training also have practical consequences. For instance, backpropagation training is a global and coupled procedure that limits the amount of possible parallelism and yields high latency.
            </p>
            <p>
              These limitations have motivated us to discuss possible alternative directions. In this workshop, we want to promote such discussions by bringing together researchers from various but related disciplines, and to discuss possible solutions from engineering, machine learning and neuroscientific perspectives.
            </p>
          <!-- </div> -->
          </div>
        </div>

      </div>
    </section><!-- #about -->


    <!--==========================
    Call To Action Section
    ============================-->
<!--     <section id="call-to-action">
      <div class="container wow fadeIn">
        <div class="row">
          <div class="col-lg-9 text-center text-lg-left">
            <h3 class="cta-title">Call For Papers</h3>
            <p class="cta-text"> 
          	We invite high-quality papers on the following topics:  </p> 
          	<div class="cta-text"> 
      			<ul>
      			<li> Large scale learning; distributed and parallel training </li>
      			<li> Statistical analysis, convergence, and generalization bounds </li>
      			<li> Neural networks with reconfigurable architectures </li>
      			<li> Alternatives to backprop such as target-prop or alternating minimization </li>
      			<li> Efficient training or inference </li>
      			<li> Biologically plausible training of deep learning models </li>
      			<li> Energy efficient deep learning </li>
      			<li> Scalable training at the system level (hardware, compilation) </li>
      			<li> Applications to new domains </li>
      			</ul> 
      			</div>

      			<p class="cta-text"> 
      			Accepted papers will be presented during the joint poster session. Exceptional submissions will be selected for short oral presentations. All the accepted papers will be publicly available as non-archival papers allowing future submissions. We also offer NeurIPS registration tickets to first authors of four selected student submissions. </p>

      			<p class="cta-text"> 
      			We expect submissions to follow the NeurIPS formatting guidelines and ask for four pages of the main content and an unlimited number of additional pages for the supplementary material and references. As an exception, we allow submissions of already-accepted papers in unchanged form, but only for the poster presentation. The CMT-based review process will be double-blind. </p>

      			<p class="cta-text"> 
      			<b>Key dates </b>	<br>		
      			Submission deadline: <strike>9th of October, 2020</strike> 12th of October 2020, anywhere on Earth <br>
      			Final decisions: <strike>30th of October, 2020</strike> 2nd of November, 2020 <br>
            </p>
          </div>
          <div class="col-lg-3 cta-btn-container text-center">
            <a class="cta-btn align-middle" href="https://cmt3.research.microsoft.com/BeyondBackProp2020">Click here to submit</a>
          </div>
        </div>

      </div>
    </section> --><!-- #call-to-action --> 

    <!--==========================
    Call To Action Section
    ============================-->
    <section id="call-to-action">
      <div class="container wow fadeIn">
        <div class="row">
         <div class="col-lg-12 content order-lg-9 order-2">
          <h2 class="cta-title"><b>Schedule</b></h2>

            <p class="cta-text">Throughout the day we will livestream invited talks, contributed talks, and live Q&A session. We will also have two virtual poster sessions in <a href="https://neurips.gather.town/app/d35oAHqQs9NyhHVV/BeyondBackprop" style="color:#7AAABE">[Gather.Town]</a>. Note, that our Gather.Town is open throughout the entire workshop, so feel free to hangout and talk to other participants! </p> 

            <p class="cta-text">All times listed are in PST. </p> 

            <p class="cta-text">If you have questions for our speakers or a topic to discuss, please post them <a href="https://app.sli.do/event/dumxcaym/live/questions" style="color:#7AAABE">[here]</a>. </p>  
            <!-- </br> -->
            <div class="cta-text"> 
<!--             <ul>
 <li> 6:00 - 6:15 - <b>Opening Remarks</b> </li>
 <li> 6:15 - 6:45 - <b>Bastiaan Veeling</b>  - Bio-Constrained Intelligence and The Great Filter <a href="https://slideslive.com/38938066/bioconstrained-intelligence-and-the-great-filter" style="color:#081B52">[Video]</a></li>
 <li> 6:45 - 7:15 - <b>Olivier Teytaud</b>  - Gradient-Free Learning (with Nevergrad) <a href="https://slideslive.com/38938068/gradientfree-learning-with-nevergrad" style="color:#081B52">[Video]</a></li>
 <li> 7:15 - 8:30 - <b>First Poster Session</b>  <a href="https://neurips.gather.town/app/d35oAHqQs9NyhHVV/BeyondBackprop" style="color:#081B52">[Video]</a></li>
 <li> 8:30 - 9:00 - <b>Karl Friston</b>  - Active Inference and Artificial Curiosity <a href="https://slideslive.com/38938067/active-inference-and-artificial-curiosity" style="color:#081B52">[Video]</a></li>
 <li> 9:00 - 9:45 - <b>Live Q&A </b> with Bastiaan Veeling, Olivier Teytaud and Karl Friston</li>
 <li> 9:45 - 11:00 - <b>Break</b> </li>
 <li> 11:00 - 11:32 - <b>Yoshua Bengio</b>  - Towards bridging the Gap between Backprop and Neuroscience <a href="https://slideslive.com/38938064/towards-bridging-the-gap-between-backdrop-and-neuroscience" style="color:#081B52">[Video]</a></li>
 <li> 11:32 - 12:08 - <b>Danielle Bassett</b>  - A Story from the Human World <a href="https://slideslive.com/38938436/a-story-from-the-human-world" style="color:#081B52">[Video]</a></li>
 <li> 12:09 - 12:40 - <b>Contributed Talks (1)</b> </li>
 <ul>
 <li> 12:10 - 12:22 - <b>Randomized Automatic Differentiation</b>  - Deniz Oktay, Nick B McGreivy, Alex Beatson, Ryan Adams <a href="https://slideslive.com/38942370/randomized-automatic-differentiation" style="color:#081B52">[Video]</a></li>
 <li> 12:22 - 12:35 - <b>ZORB: A Derivative-Free Backpropagation Algorithm for Neural Networks</b>  - Varun Ranganathan, Alex Lewandowski <a href="https://slideslive.com/38942061/zorb-a-derivativefree-backpropagation-algorithm-for-neural-networks" style="color:#081B52">[Video]</a></li>
 <li> 12:35 - 12:40 - <b>Live Q&A </b> </li>
</ul>
 <li> 12:40 - 12:45 - <b>Break</b> </li>
 <li> 12:45 - 13:15 - <b>Contributed Talks (2)</b> </li>
 <ul>
 <li> 12:46 - 12:59 - <b>Policy Manifold Search for Improving Diversity-based Neuroevolution</b>  - Nemanja Rakicevic, Antoine Cully, Petar Kormushev <a href="https://slideslive.com/38942369/policy-manifold-search-for-improving-diversitybased-neuroevolution" style="color:#081B52">[Video]</a></li>
 <li> 12:59 - 13:12 - <b>Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct Feedback Alignment</b>  - Julien Launay, Iacopo Poli, Kilian Muller, Igor Carron, Laurent Daudet, Florent Krzakala, Sylvain Gigan  <a href="https://slideslive.com/38942004/hardware-beyond-backpropagation-a-photonic-coprocessor-for-direct-feedback-alignment" style="color:#081B52">[Video]</a></li>
 <li> 13:12 - 13:15 - <b>Live Q&A </b> </li>
</ul>
 <li> 13:15 - 13:45 - <b>David Duvenaud</b>  - Hypernets and the Inverse Function Theorem <a href="https://slideslive.com/38938065/hypernets-and-the-inverse-function-theorem" style="color:#081B52">[Video]</a></li>
 <li> 13:45 - 14:15 - <b>Christina Savin</b>  - New (Old) Tricks for Online Metalearning <a href="https://slideslive.com/38938070/new-old-tricks-for-online-metalearning" style="color:#081B52">[Video]</a></li>
 <li> 14:15 - 15:00 - <b>Live Q&A</b> with Yoshua Bengio, Danielle Bassett, David Duvenaud and Christina Savin</li>
 <li> 15:00 - 16:30 - <b>Second Poster Session </b>  <a href="https://neurips.gather.town/app/d35oAHqQs9NyhHVV/BeyondBackprop" style="color:#081B52">[Video]</a></li>
            </ul>  -->

            <table>
            <tr><th style="width:8em"></th></tr>
<tr> <td> 6:00 - 6:15 </td> <td> <b>Opening Remarks</b> </td><td></td></tr>
<tr> <td> 6:15 - 6:45 </td> <td> <b>Bastiaan Veeling</b>  - Bio-Constrained Intelligence and The Great Filter</td> <td> <a href="https://slideslive.com/38938066/bioconstrained-intelligence-and-the-great-filter" style="color:#7AAABE">[Video]</a></td></tr>
<tr> <td> 6:45 - 7:15 </td> <td> <b>Olivier Teytaud</b>  - Gradient-Free Learning (with Nevergrad)</td> <td> <a href="https://slideslive.com/38938068/gradientfree-learning-with-nevergrad" style="color:#7AAABE">[Video]</a></td></tr>
<tr> <td> 7:15 - 8:30 </td> <td> <b>First Poster Session</b> </td> <td> <a href="https://neurips.gather.town/app/d35oAHqQs9NyhHVV/BeyondBackprop" style="color:#7AAABE">[Gather]</a></td></tr>
<tr> <td> 8:30 - 9:00 </td> <td> <b>Karl Friston</b>  - Active Inference and Artificial Curiosity</td> <td> <a href="https://slideslive.com/38938067/active-inference-and-artificial-curiosity" style="color:#7AAABE">[Video]</a></td></tr>
<tr> <td> 9:00 - 9:45 </td> <td> <b>Live Panel Discussion</b> with Bastiaan Veeling, Olivier Teytaud and Karl Friston</td><td></td></tr>
<tr> <td> 9:45 - 11:00 </td> <td> <b>Break</b> </td><td></td></tr>
<tr> <td> 11:00 - 11:32 </td> <td> <b>Yoshua Bengio</b>  - Towards bridging the Gap between Backprop and Neuroscience</td> <td> <a href="https://slideslive.com/38938064/towards-bridging-the-gap-between-backdrop-and-neuroscience" style="color:#7AAABE">[Video]</a></td></tr>
<tr> <td> 11:32 - 12:08 </td> <td> <b>Danielle Bassett</b>  - A Story from the Human World</td> <td> <a href="https://slideslive.com/38938436/a-story-from-the-human-world" style="color:#7AAABE">[Video]</a></td></tr>
<tr> <td> 12:09 - 12:40 </td> <td> <b>Contributed Talks (1)</b> </td><td></td></tr>
<tr> <td> 12:10 - 12:22 </td> <td> <b>Randomized Automatic Differentiation</b>  - Deniz Oktay, Nick B McGreivy, Alex Beatson, Ryan Adams</td> <td> <a href="https://slideslive.com/38942370/randomized-automatic-differentiation" style="color:#7AAABE">[Video]</a></td></tr>
<tr> <td> 12:22 - 12:35 </td> <td> <b>ZORB: A Derivative-Free Backpropagation Algorithm for Neural Networks</b>  - Varun Ranganathan, Alex Lewandowski</td> <td> <a href="https://slideslive.com/38942061/zorb-a-derivativefree-backpropagation-algorithm-for-neural-networks" style="color:#7AAABE">[Video]</a></td></tr>
<tr> <td> 12:35 - 12:40 </td> <td> <b>Live Q&A Contributed Talks (1) </b> </td><td></td></tr>
<tr> <td> 12:40 - 12:45 </td> <td> <b>Break</b> </td><td></td></tr>
<tr> <td> 12:45 - 13:15 </td> <td> <b>Contributed Talks (2)</b> </td><td></td></tr>
<tr> <td> 12:46 - 12:59 </td> <td> <b>Policy Manifold Search for Improving Diversity-based Neuroevolution</b>  - Nemanja Rakicevic, Antoine Cully, Petar Kormushev</td> <td> <a href="https://slideslive.com/38942369/policy-manifold-search-for-improving-diversitybased-neuroevolution" style="color:#7AAABE">[Video]</a></td></tr>
<tr> <td> 12:59 - 13:12 </td> <td> <b>Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct Feedback Alignment</b>  - Julien Launay, Iacopo Poli, Kilian Muller, Igor Carron, Laurent Daudet, Florent Krzakala, Sylvain Gigan </td> <td> <a href="https://slideslive.com/38942004/hardware-beyond-backpropagation-a-photonic-coprocessor-for-direct-feedback-alignment" style="color:#7AAABE">[Video]</a></td></tr>
<tr> <td> 13:12 - 13:15 </td> <td> <b>Live Q&A Contributed Talks (2) </b> </td><td></td></tr>
<tr> <td> 13:15 - 13:45 </td> <td> <b>David Duvenaud</b>  - Hypernets and the Inverse Function Theorem</td> <td> <a href="https://slideslive.com/38938065/hypernets-and-the-inverse-function-theorem" style="color:#7AAABE">[Video]</a></td></tr>
<tr> <td> 13:45 - 14:15 </td> <td> <b>Cristina Savin</b>  - New (Old) Tricks for Online Metalearning</td> <td> <a href="https://slideslive.com/38938070/new-old-tricks-for-online-metalearning" style="color:#7AAABE">[Video]</a></td></tr>
<tr> <td> 14:15 - 15:00 </td> <td> <b>Live Panel Discussion</b> with Yoshua Bengio, Danielle Bassett, David Duvenaud and Cristina Savin</td><td></td></tr>
<tr> <td> 15:00 - 16:30 </td> <td> <b>Second Poster Session </b> </td> <td> <a href="https://neurips.gather.town/app/d35oAHqQs9NyhHVV/BeyondBackprop" style="color:#7AAABE">[Gather]</a></td></tr>
            </table>
            </div>
          </div>
        </div>

      </div>
    </section>


    <!--==========================
      Speaker Section
    ============================-->
    <section id="team">
      <div id="speakers" class="container wow fadeInUp">
        <div class="section-header">
          <!-- <h2 class="title">Speakers</h2> -->
          <h3 class="section-title">Speakers</h3>
          <p class="section-description"></p>
        </div>
        <div class="row">
          <div class="col-lg-3 col-md-6">
           <div id="logo-holder">  
           <div class="member">
              <a href="https://live-sas-physics.pantheon.sas.upenn.edu/people/standing-faculty/danielle-bassett">
                <div class="pic"><img src="img/speaker/bassett.jpg" alt=""></div>
                <h4>Danielle Bassett</h4>
                <span>University of Pennsylvania</span>
              <div class="text">
               Secondary Appointment Professor of Physics and Astronomy, Primary Appointment Associate Professor of Bioengineering. <br><b>Research:</b> structure and function of networks, predominantly in physical and biological systems.
              </div>
              </a>
            </div>
          </div>
          </div>
          
          <div class="col-lg-3 col-md-6">
          <div id="logo-holder">
            <div class="member">
              <a href="https://yoshuabengio.org">
              <div class="pic"><img src="img/speaker/bengio.jpg" alt=""></div>
              <h4>Yoshua Bengio</h4>
              <span>MILA</span>
              <div class="text">
                Turing Award Winner, Professor in Computer Science at the Université de Montréal. <br><b>Research:</b> deep learning, biologically plausible learning, causality, grounding.
              </div>
            </a>
            </div>
          </div>
          </div>

          <div class="col-lg-3 col-md-6">
          <div id="logo-holder">  
            <div class="member">
              <a href="http://www.cs.toronto.edu/~duvenaud/">
                <div class="pic"><img src="img/speaker/duvenaud.jpg" alt=""></div>
                <h4>David Duvenaud</h4>
                <span>University of Toronto</span>
              <div class="text"> 
                Assistant Professor in Computer Science and Statistics at University of Toronto. <br><b>Research:</b> approximate inference, model-based optimization, Neural ODEs.
              </div>
              </a>
            </div>
          </div>
          </div>

          <div class="col-lg-3 col-md-6">
          <div id="logo-holder">  
            <div class="member">
              <a href="https://www.fil.ion.ucl.ac.uk/~karl/">
                <div class="pic"><img src="img/speaker/friston.jpg" alt=""></div>
                <h4>Karl Friston</h4>
                <span>University College London</span>
              <div class="text">
                Professor in Neurology at University College London. <br><b>Research:</b> theoretical neuroscience, brain imaging, free-energy principles.
              </div>
              <a/>
            </div>
          </div>
          </div>

          <div class="col-lg-3 col-md-6">
          <div id="logo-holder">  
            <div class="member">
              <a href="https://csavin.wixsite.com/savinlab">
                <div class="pic"><img src="img/speaker/savin.jpg" alt=""></div>
                <h4>Cristina Savin</h4>
                <span>New York University</span>
              <div class="text">
              Assistant Professor in Neuroscience and Data Science at New York University. <br><b>Research:</b> computational neuroscience,  development of computational models of memory.
              </div>
              </a>
            </div>
        </div>
        </div>

          <div class="col-lg-3 col-md-6">
            <div id="logo-holder">  
            <div class="member">
              <a href="https://www.facebook.com/notes/olivier-teytaud/optimization-machine-learning-artificial-intelligence-games-electricity/10162959845390472/">
                <div class="pic"><img src="img/speaker/teytaud.jpg" alt=""></div>
                <h4>Olivier Teytaud</h4>
                <span>Facebook</span>
              <div class="text">
                Research Scientist at Facebook. <br><b> Research:</b> gradient-free and evolutionary optimization, Nevergrad, zero-learning.
              </div>
              </a>
            </div>
          </div>
          </div>

          <div class="col-lg-3 col-md-6">
            <div id="logo-holder">  
            <div class="member">
              <a href="http://basveeling.nl">
                <div class="pic"><img src="img/speaker/veeling.jpg" alt=""></div>
                <h4>Bastiaan Veeling</h4>
                <span>University of Amsterdam</span>
              <div class="text">
                PhD student at University of Amsterdam. <br><b>Research:</b> deep learning for medical applications, local learning.
              </div>
              </a>
            </div>
          </div>
          </div>
        </div>
        </div>

      </div>
    </section><!-- #speaker -->


    <section id="team">
      <div id="organizers" class="container wow fadeInUp">
        <div class="section-header">
          <!-- <h2 class="title">Organizers</h2> -->
          <h3 class="section-title">Organizers</h3>   
          <p class="section-description"></p>

        </div>
        <div class="row">
          <div class="col-lg-3 col-md-6">
            <div id="logo-holder">  
            <div class="member">
              <a href="https://engineering.nyu.edu/faculty/anna-choromanska"> 
                 <div class="pic"><img src="img/organizers/anna.jpg" alt=""></div>
                 <h4>Anna Choroman&#769;ska</h4>
                 <span>New York University</span>
              <div class="text">
                Assistant Professor in Electrical and Computer Engineering at New York University. <br><b> Research:</b> non-convex optimization, large data analysis, robotics and autonomy.
              </div>
              </a>
               </div>
         </div>
          </div>

          <div class="col-lg-3 col-md-6">
            <div id="logo-holder">  
            <div class="member">
              <a href="http://sailab.diism.unisi.it/people/marco-gori/">
                <div class="pic"><img src="img/organizers/marco.jpg" alt=""></div>
                <h4>Marco  Gori</h4>
                <span> University of Siena</span>
              <div class="text">
                Professor in Neuroscience at the University of Siena. <br><b> Research:</b> machine learning, vision, biological learning, game playing.
              </div>
              </a>
              </div>
          </div>
          </div>

          <div class="col-lg-3 col-md-6">
            <div id="logo-holder">  
            <div class="member">
              <div class="pic"><img src="img/organizers/yanping.jpg" alt=""></div>
              <h4>Yanping Huang</h4>
              <span>Google</span>
              <div class="text">
                Research Scientist at Google Brain. <br><b> Research:</b> scalable learning, reinforcement learning, computational neuroscience.
              </div>
             </div>
           </div>
          </div>

          <div class="col-lg-3 col-md-6">
            <div id="logo-holder">  
            <div class="member">
              <a href="https://loewex.github.io/">
                <div class="pic"><img src="img/organizers/sindy.jpg" alt=""></div>
                <h4>Sindy Löwe</h4>
                <span>University of Amsterdam</span>
              <div class="text">
                PhD student at the University of Amsterdam. <br><b> Research:</b> local learning, self-supervised learning.
              </div>
              </a>
             </div>
           </div>
            </div>

            <div class="col-lg-3 col-md-6">
            <div id="logo-holder">  
            <div class="member">
              <a href="https://uk.linkedin.com/in/mateuszmalinowski">    
              <div class="pic"><img src="img/organizers/mateusz.jpg" alt=""></div>
              <h4>Mateusz Malinowski</h4>
              <span>Deepmind</span>
              <div class="text">
                Research Scientist at DeepMind. <br><b> Research:</b> reasoning, scalable learning, visual question answering, language + vision.
              </div>
              </a>
            </div>
            </div>
            </div>
            
            <div class="col-lg-3 col-md-6">
            <div id="logo-holder">  
            <div class="member">
              <div class="pic"><img src="img/organizers/viorica.jpg" alt=""></div>
              <h4>Viorica Pa&#774;tra&#774;ucean</h4>
              <span>Deepmind</span>
              <div class="text">
                Research Scientist at DeepMind. <br><b> Research:</b> computer vision, scalable learning, biologically plausible learning.
              </div>
            </div>
            </div>
            </div>
            
       		<div class="col-lg-3 col-md-6">
            <div id="logo-holder">  
            <div class="member">
              <div class="pic"><img src="img/organizers/grzegorz.jpg" alt=""></div>
              <h4>Grzegorz S&#769;wirszcz</h4>
              <span>Deepmind</span>
              <div class="text">
                 Research Scientist at DeepMind.  <br><b> Research:</b> machine learning, dynamical systems, mathematical modeling.
<!--                 <a href=""><i class="fa fa-twitter"></i></a>
                <a href=""><i class="fa fa-facebook"></i></a>
                <a href=""><i class="fa fa-google-plus"></i></a>
                <a href=""><i class="fa fa-linkedin"></i></a> -->
            </div>
              </div>
            </div>
            </div>      
          </div>
        </div>

      </div>
    </section><!-- #team -->

    <!--==========================
    Accepted Papers Section
    ============================-->
    </br>
    </br>
    </br>     
    <section id="papers">
      <div class="container">
        <div class="row about-container">
          <div class="col-lg-12 content order-lg-9 order-2">
            <h2 class="title"><b>Accepted Papers</b></h2>

            We will have two poster sessions in <a href="https://neurips.gather.town/app/d35oAHqQs9NyhHVV/BeyondBackprop" style="color:#081B52">[Gather.Town]</a>. The numbers correspond to the papers' locations during these poster sessions. See map below.
            </br></br>
   <!--  <section id="call-to-action">
      <div class="container wow fadeIn">
        <div class="row">
          <div class="col-lg-12 text-center text-lg-left"> -->
            </br>
            <h4 class="title">Papers accepted for oral presentations</h4>
            </br>
<!--             <ol>
            <li value="19"> Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct Feedback Alignment <a href="https://slideslive.com/38942004/hardware-beyond-backpropagation-a-photonic-coprocessor-for-direct-feedback-alignment?ref=account-folder-62078-folders" style="color:#081B52">[Video]</a> - <i>Julien Launay, Iacopo Poli, Kilian Müller, Igor Carron, Laurent Daudet, Florent Krzakala, Sylvain Gigan </i> </li>
            <li value="36"> Policy Manifold Search for Improving Diversity-based Neuroevolution <a href="https://slideslive.com/38942369/policy-manifold-search-for-improving-diversitybased-neuroevolution?ref=account-folder-62078-folders" style="color:#081B52">[Video]</a> - <i>Nemanja Rakicevic, Antoine Cully, Petar Kormushev </i> </li>
            <li value="39"> Randomized Automatic Differentiation <a href="https://slideslive.com/38942370/randomized-automatic-differentiation?ref=account-folder-62078-folders" style="color:#081B52">[Video]</a> - <i>Deniz Oktay, Nick B McGreivy, Joshua Aduol, Alex Beatson, Ryan P Adams </i> </li>
            <li style="list-style-type: disc"> ZORB: A Derivative-Free Backpropagation Algorithm for Neural Networks <a href="https://slideslive.com/38942061/zorb-a-derivativefree-backpropagation-algorithm-for-neural-networks?ref=account-folder-62078-folders" style="color:#081B52">[Video]</a> - <i>Varun Ranganathan, Alex Lewandowski </i> </li>
            </ol> -->
            <table>
            <tr><th style="width:2em"></th></tr>
<tr> <td>19. </td> <td>Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct Feedback Alignment - <i>Julien Launay, Iacopo Poli, Kilian Müller, Igor Carron, Laurent Daudet, Florent Krzakala, Sylvain Gigan </i></td> <td><a href="https://slideslive.com/38942004/hardware-beyond-backpropagation-a-photonic-coprocessor-for-direct-feedback-alignment?ref=account-folder-62078-folders" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>36. </td> <td>Policy Manifold Search for Improving Diversity-based Neuroevolution - <i>Nemanja Rakicevic, Antoine Cully, Petar Kormushev </i></td> <td><a href="https://slideslive.com/38942369/policy-manifold-search-for-improving-diversitybased-neuroevolution?ref=account-folder-62078-folders" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>39. </td> <td>Randomized Automatic Differentiation - <i>Deniz Oktay, Nick B McGreivy, Joshua Aduol, Alex Beatson, Ryan P Adams </i></td> <td><a href="https://slideslive.com/38942370/randomized-automatic-differentiation?ref=account-folder-62078-folders" style="color:#081B52">[Video]</a></td></tr>
<tr> <td> </td> <td>ZORB: A Derivative-Free Backpropagation Algorithm for Neural Networks - <i>Varun Ranganathan, Alex Lewandowski </i></td> <td><a href="https://slideslive.com/38942061/zorb-a-derivativefree-backpropagation-algorithm-for-neural-networks?ref=account-folder-62078-folders" style="color:#081B52">[Video]</a></td></tr>
            </table>
            </br>
            </br>

            <h4 class="title">Papers accepted for poster presentations</h4>
            </br>
            <div class="cta-text"> 
<!--             <ol>
 <li value="1"> A biologically plausible neural network for local supervision in cortical microcircuits - <i>Siavash Golkar, David Lipshutz, Yanis Bahroun, Anirvan Sengupta, Dmitri Chklovskii </i></li>
 <li value="2"> A More Biologically Plausible Local Learning Rule for ANNs - <i>Shashi Kant Gupta </i></li>
 <li value="3"> A Theoretical Framework for Target Propagation - <i>Alexander Meulemans, Francesco S Carzaniga, Johan Suykens, João Sacramento, Benjamin F. Grewe </i></li>
 <li value="4"> Align, then Select: Analysing the Learning Dynamics of Feedback Alignment - <i>Maria Refinetti, Stéphane d'Ascoli, Ruben Ohana, Sebastian Goldt</i></li>
 <li value="5"> Architecture Agnostic Neural Networks - <i>Sabera Talukder, Guruprasad Raghavan, Yisong Yue </i></li>
 <li value="6"> Backpropagation Free Transformers - <i>Dinko D Franceschi </i></li>
 <li value="7"> Biophysical Neural Networks Provide Robustness and Versatility over Artificial Neural Networks - <i>James Hazelden, Michael I Ivanitskiy, Daniel Forger </i></li>
 <li value="8"> BP2T2: Moving towards Biologically-Plausible BackPropagation Through Time - <i>Arna Ghosh, Jonathan Cornford, Blake Richards </i></li>
 <li value="9"> Convolutional Neural Networks from Image Markers <a href="https://youtu.be/mJX8xEeuTgg" style="color:#081B52">[Video]</a> - <i>Barbara C Benato, Italos Estilon de Souza, Felipe L Galvao, Alexandre X Falcão </i></li>
 <li value="10"> Deep Networks from the Principle of Rate Reduction - <i>Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, Yi Ma </i></li>
 <li value="11"> Deep Neural Network Training without Multiplications - <i>Tsuguo Mogami </i></li>
 <li value="12"> Deep Neural Networks Are Congestion Games - <i>Nina Vesseron, Ievgen Redko, Charlotte Laclau </i></li>
 <li value="13"> Deep Reservoir Networks with Learned Hidden Reservoir Weights using Direct Feedback Alignment <a href="https://youtu.be/HS4ZJf1c3rg" style="color:#081B52">[Video]</a> - <i>Matthew S Evanusa, Aloimonos Yiannis, Cornelia Fermuller </i></li>
 <li value="14"> Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures - <i>Julien Launay, François Boniface, Iacopo Poli, Florent Krzakala </i></li>
 <li value="15"> Feature Whitening via Gradient Transformation for Improved Convergence - <i>Shmulik Markovich-Golan, ‪Barak Battash‬‏, Amit Bleiweiss </i></li>
 <li value="16"> Front Contribution instead of Back Propagation <a href="https://youtu.be/zOwab48CvFk" style="color:#081B52">[Video]</a> - <i>Swaroop Ranjan Mishra, Anjana Arunkumar </i></li>
 <li value="17"> Gated Linear Networks and Extensions - <i>Eren Sezener, David Budden, Marcus Hutter, Christopher Mattern, Jianan Wang, Joel Veness </i></li>
 <li value="18"> Generalized Stochastic Backpropagation - <i>Amine Echraibi, Joachim Flocon-Cholet, Stéphane W Gosselin, Sandrine Vaton </i></li>
 <li value="20"> HebbNet: A Simplified Hebbian Learning Framework to do Biologically Plausible Learning <a href="https://youtu.be/6P0WpEI515E" style="color:#081B52">[Video]</a> - <i>Manas Gupta, ArulMurugan Ambikapathi, Ramasamy Savitha </i></li>
 <li value="21"> Hindsight Network Credit Assignment <a href="https://youtu.be/3GvvdnTXtdQ" style="color:#081B52">[Video]</a> - <i>Kenny Young </i></li>
 <li value="22"> How and When does Feedback Alignment Work? - <i>Stéphane d'Ascoli, Maria Refinetti, Ruben Ohana, Sebastian Goldt</i></li>
 <li value="23"> Ignorance is Bliss: Adversarial Robustness by Design through Analog Computing and Synaptic Asymmetry - <i>Alessandro Cappelli, Ruben Ohana, Julien Launay, Iacopo Poli, Florent Krzakala </i></li>
 <li value="24"> Improving Multimodal Accuracy Through Modality Pre-training and Attention - <i>Aya Abdelsalam A Ismail, Faisal Ishtiaq, Mahmudul Hasan </i></li>
 <li value="25"> Investigating Coagent Networks for Supervised Learning - <i>Dhawal Gupta, Matthew Schlegel, James Kostas, Gabor Mihucz, Martha White </i></li>
 <li value="26"> Investigating the Scalability and Biological Plausibility of the Activation Relaxation Algorithm - <i>Beren Millidge, Alexander D Tschanz, Anil Seth, Christopher Buckley </i></li>
 <li value="27"> Layer-wise Learning of Kernel Dependence Networks <a href="https://youtu.be/usvePLCxEjs" style="color:#081B52">[Video]</a> - <i>Chieh Tzu Wu, Aria Masoomi, Arthur Gretton, Jennifer Dy </i></li>
 <li value="28"> Layer-wise Learning via Kernel Embedding <a href="https://youtu.be/usvePLCxEjs" style="color:#081B52">[Video]</a> - <i>Aria Masoomi, Chieh Tzu Wu, Arthur Gretton, Jennifer Dy </i></li>
 <li value="29"> Learning Flows By Parts <a href="https://youtu.be/6fpKtF41_Tc" style="color:#081B52">[Video]</a> - <i>Manush Bhatt, David I Inouye </i></li>
 <li value="30"> MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks - <i>Zhiqiang Shen, Marios Savvides </i></li>
 <li value="31"> Menger: Large-Scale Distributed Reinforcement Learning - <i>Amir Yazdanbakhsh, Junchao Chen, Yu Zheng </i></li>
 <li value="32"> Meta-Learning Backpropagation And Improving It <a href="https://youtu.be/nkTQGU265rk" style="color:#081B52">[Video]</a> - <i>Louis Kirsch, Jürgen Schmidhuber </i></li>
 <li value="33"> MPLP: Learning a Message Passing Learning Protocol <a href="https://youtu.be/hSel-1JrLJ0" style="color:#081B52">[Video]</a> - <i>Ettore Randazzo, Eyvind Niklasson, Alexander Mordvintsev </i></li>
 <li value="34"> Neighbourhood Distillation: On the benefits of non end-to-end distillation - <i>Laëtitia M Shao, Max Moroz, Elad Eban, Yair Movshovitz-Attias </i></li>
 <li value="35"> Optimizing Neural Networks via Koopman Operator Theory - <i>William T Redman, Akshunna S. Dogra </i></li>
 <li value="37"> Predicting Pretrained Weights of Large-scale CNNs <a href="https://youtu.be/1wwY0tSxZ0E" style="color:#081B52">[Video]</a> - <i>Boris Knyazev, Michal Drozdzal, Graham Taylor, Adriana Romero </i></li>
 <li value="38"> PyTorch-Hebbian: facilitating local learning in a deep learning framework - <i>Jules Talloen </i></li>
 <li value="40"> Scaling Equilibrium Propagation to Deep ConvNets by Drastically Reducing its Gradient Estimator Bias <a href="https://youtu.be/K0BAHHyLwZg" style="color:#081B52">[Video]</a> - <i>Axel Laborieux, Maxence M ERNOULT, Benjamin Scellier, Yoshua Bengio, Julie Grollier, Damien Querlioz </i></li>
 <li value="41"> Scaling up learning with GAIT-prop - <i>Sander Dalm </i></li>
 <li value="42"> Self Normalizing Flows <a href="https://youtu.be/6Q3b3MergqI" style="color:#081B52">[Video]</a> - <i>Thomas A Keller, Jorn W.T. Peters, Priyank Jaini, Emiel Hoogeboom, Patrick Forré, Max Welling </i></li>
 <li value="43"> Short-Term Memory Optimization in Recurrent Neural Networks by Autoencoder-based Initialization <a href="https://youtu.be/x4zLMQImScY" style="color:#081B52">[Video]</a> - <i>Antonio Carta, Alessandro Sperduti, Davide Bacciu </i></li>
 <li value="44"> Slot Machines: Discovering Winning Combinations of Random Weights in Neural Networks - <i>Maxwell M Aladago, Lorenzo Torresani </i></li>
 <li value="45"> Supervised Learning with Brain Assemblies <a href="https://youtu.be/t9T7bfdYUwU" style="color:#081B52">[Video]</a> - <i>Akshay Rangamani, Anshula Gandhi </i></li>
 <li value="46"> Symbiotic Learning of Dual Discrimination andReconstruction Networks - <i>Tahereh Toosi, Elias B Issa </i></li>
 <li value="47"> The Interplay of Search and Gradient Descent in Semi-stationary Learning Problems <a href="https://youtu.be/upyw2Dyh-q4" style="color:#081B52">[Video]</a> - <i>Shibhansh Dohare, Rupam Mahmood, Richard S Sutton </i></li>
 <li value="48"> Towards self-certified learning: Probabilistic neural networks trained by PAC-Bayes with Backprop - <i>Maria Perez-Ortiz, Omar Rivasplata, John Shawe-Taylor, Csaba Szepesvari </i></li>
 <li value="49"> Towards truly local gradients with CLAPP: Contrastive, Local And Predictive Plasticity <a href="https://youtu.be/pZjoCZFpRf4" style="color:#081B52">[Video]</a> - <i>Bernd Illing, Guillaume Bellec, Wulfram Gerstner </i></li>
 <li value="50"> Unintended Effects on Adaptive Learning Rate for Training Neural Network with Output Scale Change <a href="https://youtu.be/odtdBUtv4pw" style="color:#081B52">[Video]</a> - <i>Ryuichi Kanoh, Mahito Sugiyama </i></li>
            </ol> 
            </div> -->
            <table>
             <tr><th style="width:2em"></th></tr>
<tr> <td>1. </td> <td>A biologically plausible neural network for local supervision in cortical microcircuits - <i>Siavash Golkar, David Lipshutz, Yanis Bahroun, Anirvan Sengupta, Dmitri Chklovskii </i></td> <td></td></tr>
<tr> <td>2. </td> <td>A More Biologically Plausible Local Learning Rule for ANNs - <i>Shashi Kant Gupta </i></td> <td></td></tr>
<tr> <td>3. </td> <td>A Theoretical Framework for Target Propagation - <i>Alexander Meulemans, Francesco S Carzaniga, Johan Suykens, João Sacramento, Benjamin F. Grewe </i></td> <td></td></tr>
<tr> <td>4. </td> <td>Align, then Select: Analysing the Learning Dynamics of Feedback Alignment - <i>Maria Refinetti, Stéphane d'Ascoli, Ruben Ohana, Sebastian Goldt</i></td> <td></td></tr>
<tr> <td>5. </td> <td>Architecture Agnostic Neural Networks - <i>Sabera Talukder, Guruprasad Raghavan, Yisong Yue </i></td> <td></td></tr>
<tr> <td>6. </td> <td>Backpropagation Free Transformers - <i>Dinko D Franceschi </i></td> <td></td></tr>
<tr> <td>7. </td> <td>Biophysical Neural Networks Provide Robustness and Versatility over Artificial Neural Networks - <i>James Hazelden, Michael I Ivanitskiy, Daniel Forger </i></td> <td></td></tr>
<tr> <td>8. </td> <td>BP2T2: Moving towards Biologically-Plausible BackPropagation Through Time - <i>Arna Ghosh, Jonathan Cornford, Blake Richards </i></td> <td></td></tr>
<tr> <td>9. </td> <td>Convolutional Neural Networks from Image Markers - <i>Barbara C Benato, Italos Estilon de Souza, Felipe L Galvao, Alexandre X Falcão </i></td> <td> <a href="https://youtu.be/mJX8xEeuTgg" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>10. </td> <td>Deep Networks from the Principle of Rate Reduction - <i>Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, Yi Ma </i></td> <td></td></tr>
<tr> <td>11. </td> <td>Deep Neural Network Training without Multiplications - <i>Tsuguo Mogami </i></td> <td></td></tr>
<tr> <td>12. </td> <td>Deep Neural Networks Are Congestion Games - <i>Nina Vesseron, Ievgen Redko, Charlotte Laclau </i></td> <td></td></tr>
<tr> <td>13. </td> <td>Deep Reservoir Networks with Learned Hidden Reservoir Weights using Direct Feedback Alignment - <i>Matthew S Evanusa, Aloimonos Yiannis, Cornelia Fermuller </i></td> <td> <a href="https://youtu.be/HS4ZJf1c3rg" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>14. </td> <td>Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures - <i>Julien Launay, François Boniface, Iacopo Poli, Florent Krzakala </i></td> <td></td></tr>
<tr> <td>15. </td> <td>Feature Whitening via Gradient Transformation for Improved Convergence - <i>Shmulik Markovich-Golan, ‪Barak Battash‬‏, Amit Bleiweiss </i></td> <td></td></tr>
<tr> <td>16. </td> <td>Front Contribution instead of Back Propagation - <i>Swaroop Ranjan Mishra, Anjana Arunkumar </i></td> <td> <a href="https://youtu.be/zOwab48CvFk" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>17. </td> <td>Gated Linear Networks and Extensions - <i>Eren Sezener, David Budden, Marcus Hutter, Christopher Mattern, Jianan Wang, Joel Veness </i></td> <td></td></tr>
<tr> <td>18. </td> <td>Generalized Stochastic Backpropagation - <i>Amine Echraibi, Joachim Flocon-Cholet, Stéphane W Gosselin, Sandrine Vaton </i></td> <td></td></tr>
<tr> <td>20. </td> <td>HebbNet: A Simplified Hebbian Learning Framework to do Biologically Plausible Learning - <i>Manas Gupta, ArulMurugan Ambikapathi, Ramasamy Savitha </i></td> <td> <a href="https://youtu.be/6P0WpEI515E" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>21. </td> <td>Hindsight Network Credit Assignment - <i>Kenny Young </i></td> <td> <a href="https://youtu.be/3GvvdnTXtdQ" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>22. </td> <td>How and When does Feedback Alignment Work? - <i>Stéphane d'Ascoli, Maria Refinetti, Ruben Ohana, Sebastian Goldt</i></td> <td></td></tr>
<tr> <td>23. </td> <td>Ignorance is Bliss: Adversarial Robustness by Design through Analog Computing and Synaptic Asymmetry - <i>Alessandro Cappelli, Ruben Ohana, Julien Launay, Iacopo Poli, Florent Krzakala </i></td> <td></td></tr>
<tr> <td>24. </td> <td>Improving Multimodal Accuracy Through Modality Pre-training and Attention - <i>Aya Abdelsalam A Ismail, Faisal Ishtiaq, Mahmudul Hasan </i></td> <td></td></tr>
<tr> <td>25. </td> <td>Investigating Coagent Networks for Supervised Learning - <i>Dhawal Gupta, Matthew Schlegel, James Kostas, Gabor Mihucz, Martha White </i></td> <td></td></tr>
<tr> <td>26. </td> <td>Investigating the Scalability and Biological Plausibility of the Activation Relaxation Algorithm - <i>Beren Millidge, Alexander D Tschanz, Anil Seth, Christopher Buckley </i></td> <td></td></tr>
<tr> <td>27. </td> <td>Layer-wise Learning of Kernel Dependence Networks - <i>Chieh Tzu Wu, Aria Masoomi, Arthur Gretton, Jennifer Dy </i></td> <td> <a href="https://youtu.be/usvePLCxEjs" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>28. </td> <td>Layer-wise Learning via Kernel Embedding - <i>Aria Masoomi, Chieh Tzu Wu, Arthur Gretton, Jennifer Dy </i></td> <td> <a href="https://youtu.be/usvePLCxEjs" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>29. </td> <td>Learning Flows By Parts - <i>Manush Bhatt, David I Inouye </i></td> <td> <a href="https://youtu.be/6fpKtF41_Tc" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>30. </td> <td>MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks - <i>Zhiqiang Shen, Marios Savvides </i></td> <td></td></tr>
<tr> <td>31. </td> <td>Menger: Large-Scale Distributed Reinforcement Learning - <i>Amir Yazdanbakhsh, Junchao Chen, Yu Zheng </i></td> <td></td></tr>
<tr> <td>32. </td> <td>Meta-Learning Backpropagation And Improving It - <i>Louis Kirsch, Jürgen Schmidhuber </i></td> <td> <a href="https://youtu.be/nkTQGU265rk" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>33. </td> <td>MPLP: Learning a Message Passing Learning Protocol - <i>Ettore Randazzo, Eyvind Niklasson, Alexander Mordvintsev </i></td> <td> <a href="https://youtu.be/hSel-1JrLJ0" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>34. </td> <td>Neighbourhood Distillation: On the benefits of non end-to-end distillation - <i>Laëtitia M Shao, Max Moroz, Elad Eban, Yair Movshovitz-Attias </i></td> <td></td></tr>
<tr> <td>35. </td> <td>Optimizing Neural Networks via Koopman Operator Theory - <i>William T Redman, Akshunna S. Dogra </i></td> <td></td></tr>
<tr> <td>37. </td> <td>Predicting Pretrained Weights of Large-scale CNNs - <i>Boris Knyazev, Michal Drozdzal, Graham Taylor, Adriana Romero </i></td> <td> <a href="https://youtu.be/1wwY0tSxZ0E" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>38. </td> <td>PyTorch-Hebbian: facilitating local learning in a deep learning framework - <i>Jules Talloen </i></td> <td></td></tr>
<tr> <td>40. </td> <td>Scaling Equilibrium Propagation to Deep ConvNets by Drastically Reducing its Gradient Estimator Bias - <i>Axel Laborieux, Maxence M ERNOULT, Benjamin Scellier, Yoshua Bengio, Julie Grollier, Damien Querlioz </i></td> <td> <a href="https://youtu.be/K0BAHHyLwZg" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>41. </td> <td>Scaling up learning with GAIT-prop - <i>Sander Dalm </i></td> <td></td></tr>
<tr> <td>42. </td> <td>Self Normalizing Flows - <i>Thomas A Keller, Jorn W.T. Peters, Priyank Jaini, Emiel Hoogeboom, Patrick Forré, Max Welling </i></td> <td> <a href="https://youtu.be/6Q3b3MergqI" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>43. </td> <td>Short-Term Memory Optimization in Recurrent Neural Networks by Autoencoder-based Initialization - <i>Antonio Carta, Alessandro Sperduti, Davide Bacciu </i></td> <td> <a href="https://youtu.be/x4zLMQImScY" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>44. </td> <td>Slot Machines: Discovering Winning Combinations of Random Weights in Neural Networks - <i>Maxwell M Aladago, Lorenzo Torresani </i></td> <td></td></tr>
<tr> <td>45. </td> <td>Supervised Learning with Brain Assemblies - <i>Akshay Rangamani, Anshula Gandhi </i></td> <td> <a href="https://youtu.be/t9T7bfdYUwU" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>46. </td> <td>Symbiotic Learning of Dual Discrimination andReconstruction Networks - <i>Tahereh Toosi, Elias B Issa </i></td> <td></td></tr>
<tr> <td>47. </td> <td>The Interplay of Search and Gradient Descent in Semi-stationary Learning Problems - <i>Shibhansh Dohare, Rupam Mahmood, Richard S Sutton </i></td> <td> <a href="https://youtu.be/upyw2Dyh-q4" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>48. </td> <td>Towards self-certified learning: Probabilistic neural networks trained by PAC-Bayes with Backprop - <i>Maria Perez-Ortiz, Omar Rivasplata, John Shawe-Taylor, Csaba Szepesvari </i></td> <td></td></tr>
<tr> <td>49. </td> <td>Towards truly local gradients with CLAPP: Contrastive, Local And Predictive Plasticity - <i>Bernd Illing, Guillaume Bellec, Wulfram Gerstner </i></td> <td> <a href="https://youtu.be/pZjoCZFpRf4" style="color:#081B52">[Video]</a></td></tr>
<tr> <td>50. </td> <td>Unintended Effects on Adaptive Learning Rate for Training Neural Network with Output Scale Change - <i>Ryuichi Kanoh, Mahito Sugiyama </i></td> <td> <a href="https://youtu.be/odtdBUtv4pw" style="color:#081B52">[Video]</a></td></tr>
            </table>

            </br>
            <p class="cta-text"> 
            Accepted papers will be presented during the poster sessions in <a href="https://neurips.gather.town/app/d35oAHqQs9NyhHVV/BeyondBackprop" style="color:#081B52">[Gather.Town]</a>. This is the map: </p>
            <div class="pic"><img class="map" src="img/map.png" alt=""></div>
            </br>
            <a href="https://docs.google.com/document/d/1iBtvpvb7Pm7VSV9MM3wH_7yGMOHQpF2XrUHagYvQqkI/edit?usp=sharing" style="color:#081B52">[This document]</a> provides guidelines on the Gather.Town virtual platform used for the Beyond Backpropagation Workshop.
            </br>
            </br>
            </br>
            </br>
            <h2 class="title"><b>Reviewers</b></h2>   
          </br>
            <!-- <h3 class="cta-title"><b>Reviewers</b></h3> -->
            <p class="cta-text"> 
            We would like to thank all of our reviewers for their great work: </br></br>
            Andrea Zugarini,
            Apoorva Nandini Saridena,
            Devansh Bisla, 
            Enrico Meloni,
            Francesco Giannini,
            Gabriele Ciravegna, 
            Giovanna Dimitri,
            Giuseppe  D'Inverno,
            Giuseppe Marra,
            Haoran Zhu,
            Jing Wang,
            Joao Carreira,
            Lapo Faggi,
            Laura Sanabria-Rosas,
            Lisa Graziani,
            Maryam Majzoubi,
            Shihong Fang,
            Takieddine Mekhalfa,
            Yunfei Teng
          </p>
            </br>
            </br>
            </br>
            </br>
        </div>
      </div>
      </div>
    </section><!-- #accepted papers -->


  </main>

  <!--==========================
    Footer
  ============================-->
  <footer id="footer">
    <div class="footer-top">
      <div class="container">

      </div>
    </div>

    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong>BeyondBackpropagation</strong>. All Rights Reserved
      </div>
      <div class="credits">
        <!--
          All the links in the footer should remain intact.
          You can delete the links only if you purchased the pro version.
          Licensing information: https://bootstrapmade.com/license/
          Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/buy/?theme=Regna
        -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>
  </footer><!-- #footer -->

  <a href="#" class="back-to-top"><i class="fa fa-chevron-up"></i></a>

  <!-- JavaScript Libraries -->
  <script src="lib/jquery/jquery.min.js"></script>
  <script src="lib/jquery/jquery-migrate.min.js"></script>
  <script src="lib/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="lib/easing/easing.min.js"></script>
  <script src="lib/wow/wow.min.js"></script>
  <script src="lib/waypoints/waypoints.min.js"></script>
  <script src="lib/counterup/counterup.min.js"></script>
  <script src="lib/superfish/hoverIntent.js"></script>
  <script src="lib/superfish/superfish.min.js"></script>

  <!-- Contact Form JavaScript File -->
  <script src="contactform/contactform.js"></script>

  <!-- Template Main Javascript File -->
  <script src="js/main.js"></script>

</body>
</html>
